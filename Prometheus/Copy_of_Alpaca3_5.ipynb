{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermegranchopro/Prometheus/blob/main/Prometheus/Copy_of_Alpaca3_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalar os requeriemntos do Alpaca API"
      ],
      "metadata": {
        "id": "QNdv0UPHj57_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install alpaca-trade-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXOskRT-coC_",
        "outputId": "9543abb4-ef70-4269-993d-bc984849501f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (2.32.3)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.26.20)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (3.11.11)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18.1->alpaca-trade-api) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso o Yahoo Finance API para saber quando é que a empresa que estou a usar entrou no mercado, quando a empresa IPO."
      ],
      "metadata": {
        "id": "fn-Uyl5AkDkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "\n",
        "# Defina o símbolo da empresa\n",
        "ticker_symbol = 'AAPL'  # Exemplo com Apple\n",
        "\n",
        "# Obtenha os dados históricos\n",
        "ticker_data = yf.Ticker(ticker_symbol)\n",
        "historical_data = ticker_data.history(period='max')\n",
        "\n",
        "# Obtenha a data da primeira cotação\n",
        "first_trading_date = historical_data.index[0]\n",
        "print(f\"A empresa {ticker_symbol} começou a ser cotada na bolsa em:\", first_trading_date.strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "def get_start_date(symbol):\n",
        "    ticker_data = yf.Ticker(symbol)\n",
        "    historical_data = ticker_data.history(period='max')\n",
        "    first_trading_date = historical_data.index[0]\n",
        "    return first_trading_date"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NibJeBQn8Jz",
        "outputId": "9033e032-6e86-42e5-a691-8eee2ca40adc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A empresa AAPL começou a ser cotada na bolsa em: 1980-12-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saber o dia que é hoje, o dia que o código está a correr, o modelo está a ser criado"
      ],
      "metadata": {
        "id": "aCpXDW-mkwqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "# Obter a data e hora atuais\n",
        "current_date_time = datetime.now()\n",
        "\n",
        "# Formatar a data como string\n",
        "current_date = current_date_time.strftime(\"%Y-%m-%d\")\n",
        "print(f\"Hoje é: {current_date}\")\n",
        "\n",
        "def get_end_date(symbol):\n",
        "    # Obter a data e hora atuais\n",
        "    current_date_time = datetime.now()\n",
        "    # Make start timezone aware\n",
        "    current_date_time = current_date_time.replace(tzinfo=timezone.utc)\n",
        "    return current_date_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-FBJmdfoKD8",
        "outputId": "8350566a-a04c-4c36-8f29-6f795d5cbac7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoje é: 2025-01-27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usas a tua senha da tua conta no ALpca API para usares a API, tens de te conectar á tua conta."
      ],
      "metadata": {
        "id": "e4cmStbtk8iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import alpaca_trade_api as tradeapi\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def alpaca_autentification():\n",
        "    # Replace with your Alpaca API credentials\n",
        "    API_KEY = 'AK1RX6F8W6QX207XPLDF'\n",
        "    SECRET_KEY = 'WaPoTTxkQBGzC51LajCdyw8Pl6svbINa9eDu9TMK'\n",
        "    BASE_URL = 'https://api.alpaca.markets'  # Use 'https://api.alpaca.markets' for live trading\n",
        "\n",
        "    # Initialize the Alpaca API\n",
        "    api = tradeapi.REST(API_KEY, SECRET_KEY, BASE_URL, api_version='v2')\n",
        "\n",
        "    return api"
      ],
      "metadata": {
        "id": "JLN6w0ThPf2r"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois chamas a API para retirar os dados que necessitas. Para a API não te bloquear usas um intervalo de 1 segundo sempre que chamas a API para conseguires atingir elevada granuralidade dos dados, sem seres bloqueado pela API."
      ],
      "metadata": {
        "id": "EBvR8uldPn23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "eTHSAnj2VD7q"
      },
      "outputs": [],
      "source": [
        "# Function to fetch data in chunks to avoid rate limits\n",
        "def fetch_data(symbol, start, end, timeframe, api):\n",
        "    all_data = []\n",
        "    current_start = start\n",
        "\n",
        "    while current_start < end:\n",
        "        current_end = current_start + timedelta(days=7)\n",
        "        if current_end > end:\n",
        "            current_end = end\n",
        "\n",
        "        # Fetch the historical data\n",
        "        bars = api.get_bars(\n",
        "            symbol,\n",
        "            timeframe,\n",
        "            start=current_start.strftime('%Y-%m-%d'),\n",
        "            end=current_end.strftime('%Y-%m-%d')\n",
        "        ).df\n",
        "\n",
        "        all_data.append(bars)\n",
        "        current_start = current_end\n",
        "        time.sleep(1)  # Adjust sleep interval based on rate limits\n",
        "\n",
        "    return pd.concat(all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalas a biblioteca necessária para calculares o garu necessário de semelhança entre strings"
      ],
      "metadata": {
        "id": "2Gm5rqxKmLDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1KpLbgPBeZw",
        "outputId": "0c1d38a0-c0eb-4be0-aeeb-5920803987a2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.11/dist-packages (0.26.1)\n",
            "Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.11/dist-packages (from python-Levenshtein) (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalas a biblioteca necessária para calculares o garu necessário de semelhança entre strings"
      ],
      "metadata": {
        "id": "TgkbUA5dmXsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhwnmPwyBLn8",
        "outputId": "7639b5b1-4177-4652-903b-ec7bd40f59df"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.11/dist-packages (0.18.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecta com a tua google drive"
      ],
      "metadata": {
        "id": "Xs8gnneeyAno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fvmqaTcxDCBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b516a931-111f-42af-be1e-a280ed736b93"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "função para criar o nome do ficheiro. O nome do ficheiro encriptará todas varaiveis/hyperparametros necessários para voltar a correr o ficheiro do modelo ou o csv com todos os dados. (tens um dicionário do que cada uma dos paramteros representa mais a baixo)"
      ],
      "metadata": {
        "id": "fm4FGj4syKlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_name(symbol, max_period, start_date, end_date,\n",
        "                     timeframe, fisrt_marker, second_marker, third_marker,\n",
        "                     n_rows, sort, relative_first_marker, relative_second_marker,\n",
        "                     relative_third_marker, decimals, LSTM_boolean, epochs,\n",
        "                     early_stopping, call_back, patience, batch_size,\n",
        "                     test_loss, test_accuracy, model_file):\n",
        "\n",
        "    # Nome aproximado do arquivo\n",
        "    if model_file == True:\n",
        "        nome_aproximado = 's=' + symbol + '+' + 'mp=' + str(max_period) + '+' + 'sd=' + \\\n",
        "        start_date + '+' + 'ed=' + end_date + '+' + 'tf=' + timeframe + '+' + \\\n",
        "        'fm=' + fisrt_marker + '+' + 'sm=' + second_marker + '+' + \\\n",
        "        'tm=' + third_marker + '+' + 'r=' + str(n_rows) + '+' + \\\n",
        "        'sort=' + str(sort) + '+' + 'rfm=' + str(relative_first_marker) + '+' + \\\n",
        "        'rsm=' + str(relative_second_marker) + '+' + \\\n",
        "        'rtm=' + str(relative_third_marker) + '+' + \\\n",
        "        'd=' + decimals + '+' + 'Lb=' + str(LSTM_boolean) + '+' + 'e=' + str(epochs) + '+' + \\\n",
        "        'es=' + str(early_stopping) + '+' + 'cb=' + call_back + '+' + \\\n",
        "        'p=' + str(patience) + '+' + 'bs=' + str(batch_size) + '+' + \\\n",
        "        'tl=' + str(test_loss) + '+' + 'ta=' + str(test_accuracy) + '.keras'\n",
        "    else:\n",
        "        nome_aproximado = 's=' + symbol + '+' + 'mp=' + str(max_period) + '+' + 'sd=' + \\\n",
        "        start_date + '+' + 'ed=' + end_date + '+' + 'tf=' + timeframe + '.csv'\n",
        "\n",
        "    return nome_aproximado"
      ],
      "metadata": {
        "id": "yJpHTFerYIzx"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "função para procurar se o modelo e os dados são originais, caso não sejamm originais é porque foi encontrado dados e pelo menos um modelo identicos ao que está a ser pedido ao algoritmo já guradado na base de dados e caso isso aconteça, os dados não seram novamente retirados da API mas sim seram usados aqueles que já se encontram na base de dados (arquivos da tua google drive). Isto é feito por compração de nomes de ficheiros, já que no nome dos ficheiros encotra-se encriptado tudo o que é necessário para voltar a correr o algoritmo em termos de hyperparamtros e variaveis."
      ],
      "metadata": {
        "id": "mhnplKm7zhWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "def old_dataset_function(symbol, max_period, start_date, end_date,\n",
        "                timeframe, fisrt_marker, second_marker, third_marker,\n",
        "                n_rows, sort, relative_first_marker, relative_second_marker,\n",
        "                relative_third_marker, decimals, LSTM_boolean, epochs,\n",
        "                early_stopping, call_back, patience, batch_size,\n",
        "                test_loss, test_accuracy, model_file):\n",
        "\n",
        "    # Defina o caminho do diretório no Google Drive\n",
        "    diretorio = '/content/drive/MyDrive/AI Financial Analisys/Summer Project/Dataset/Raw Data'\n",
        "\n",
        "    # Nome aproximado do arquivo\n",
        "    nome_aproximado = create_file_name(symbol, max_period, start_date, end_date,\n",
        "                                      timeframe, fisrt_marker, second_marker, third_marker,\n",
        "                                      n_rows, sort, relative_first_marker, relative_second_marker,\n",
        "                                      relative_third_marker, decimals, LSTM_boolean, epochs,\n",
        "                                      early_stopping, call_back, patience, batch_size,\n",
        "                                      test_loss, test_accuracy, model_file)\n",
        "\n",
        "    # Lista todos os arquivos no diretório\n",
        "    arquivos = os.listdir(diretorio)\n",
        "\n",
        "    # Encontra o arquivo mais próximo\n",
        "    if process.extractOne(nome_aproximado, arquivos):\n",
        "        arquivo_mais_proximo, pontuacao = process.extractOne(nome_aproximado, arquivos)\n",
        "    else:\n",
        "        arquivo_mais_proximo = None\n",
        "        pontuacao = None\n",
        "\n",
        "    if arquivo_mais_proximo and pontuacao==100:\n",
        "        caminho_do_modelo = os.path.join(diretorio, arquivo_mais_proximo)\n",
        "        print(\"Nome do arquivo mais próximo:\")\n",
        "        print(arquivo_mais_proximo)\n",
        "        print()\n",
        "        print(\"Caminho completo do modelo:\")\n",
        "        print(caminho_do_modelo)\n",
        "        print()\n",
        "        print(\"Pontuação de similaridade:\")\n",
        "        print(pontuacao)\n",
        "        old_dataset = True\n",
        "    else:\n",
        "        print(\"Nenhum arquivo próximo encontrado.\")\n",
        "        caminho_do_modelo = ''\n",
        "        old_dataset = False\n",
        "\n",
        "    return old_dataset, caminho_do_modelo"
      ],
      "metadata": {
        "id": "H1aTgw85yU4x"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando a alpaca API para sacar os dados agora que já deste login á tua conta da alpaca com a tua senha pessoal anteriormente."
      ],
      "metadata": {
        "id": "vlH8R_Wz3rIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_data(symbol = 'AAPL', max_period = True, start_date = '', end_date= '', timeframe = '1Min'):\n",
        "    api = alpaca_autentification()\n",
        "\n",
        "    if max_period and start_date == '' and end_date == '':\n",
        "        start_date = get_start_date(symbol)\n",
        "        end_date = get_end_date(symbol)\n",
        "    elif start_date != '' and end_date != '':\n",
        "        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    else:\n",
        "        start_date = datetime(2022, 12, 31)\n",
        "        end_date = datetime(2023, 12, 31)\n",
        "\n",
        "    # Fetch the historical data\n",
        "    apple_data = fetch_data(symbol, start_date, end_date, timeframe, api)\n",
        "    return apple_data"
      ],
      "metadata": {
        "id": "hj2WI5PG1nTV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função irá preparar dos dados para o modelo. Do Alpca seram retirados multiplos parametros do mercado interessa-te selecionar apenas os que queres alimentar ao modelo para isso esat função é usada para os selecionar. podes escolher entre 'close', 'high', 'low', 'trade_count', 'open', 'volume', 'vwap', que são os parametros de mercado dados por default pelo Alpaca. Mas para além disso crias te outras 2 métricas com base nas métricas já dadas caso as queiras testar eslas são: 'average_oc' e 'average_hl', que correspondem, respetivamente, ao ponto médio entre o open e o close e o ponto médio entre o high e o low.\n",
        "\n",
        "\n",
        "\n",
        "*   close é o preço de fecho da ação naquele intervalo temporal\n",
        "*   high é o preço mais alto da ação naquele intervalo temporal\n",
        "*   low é o preço mais baixo da ação naquele intervalo temporal\n",
        "*   trade_count é o número de matches entre compradosres e vendedores naquele intervalo temporal\n",
        "*   open é o preço de abertura da ação naquele intervalo temporal\n",
        "*   vwap é o preço da ação por volume naquele intervalo temporal ( uma forma diferente de olhar para o valor intrinseco de uma ação)\n",
        "\n"
      ],
      "metadata": {
        "id": "UCNj32r838K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare the data\n",
        "def prepare_data(data, fisrt_marker, second_marker, third_marker):\n",
        "    # Selecionar colunas relevantes e fazer uma cópia para evitar advertências\n",
        "    data_selected = data[['close', 'high', 'low', 'trade_count', 'open', 'volume', 'vwap']].copy()\n",
        "\n",
        "    # Calcular a média entre 'Open' e 'Close' usando .loc para evitar o SettingWithCopyWarning\n",
        "    data_selected.loc[:, 'average_oc'] = data_selected[['open', 'close']].mean(axis=1)\n",
        "\n",
        "    # Calcular a média entre 'High' e 'Low' usando .loc para evitar o SettingWithCopyWarning\n",
        "    data_selected.loc[:, 'average_hl'] = data_selected[['high', 'low']].mean(axis=1)\n",
        "\n",
        "    # Selecionar as colunas finais\n",
        "    if third_marker != '':\n",
        "        data_final = data_selected[[fisrt_marker, second_marker, third_marker]]\n",
        "    else:\n",
        "        data_final = data_selected[[fisrt_marker, second_marker]]\n",
        "\n",
        "    return data_final"
      ],
      "metadata": {
        "id": "1ktLcRlNC-s4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora temos de continuara a preparação dos dados para serem alimentado ao modelo para isso crio matrizes para segmentar os dados, cada matriz vai ter uma label: 0 ou 1. Assim o dataset vai ser constituido por ,atrizes de tamanho a defenir pelo o usuário do algoritmo e pelas suas respetivas labels de 0 ou 1. As matrizes no máximo poderão ter até 3 colunas, cada coluna representa uma metrica difrente do mercado. As linhas estam inicialmente organizadas de forma sequencial e representação a cada uma a granuralidade mínima do dataset, assim se a granuralidade é de 1 minuto e uma matriz tem 5 linhas, cada matriz representa 5 minutos continuos de métricas do mercado no tempo. As métricas de cada matriz por coluna podem ser formatadas como relativas, o que significa que as métricas da matriz seram apresentadas de forma relativa á matriz em questão, o que pode ser interessante para modelos sem ser um LSTM, pois permite que os dados históricos sejam baralhados temporalmente e manetenham coerência. Como é o caso por exemplo do preço e do volume, com a inflação é normal que o preço aumente mas não queremos que isso cause uma precepção errada de magnitude superior ao nosso modelo assim o 'relative' faz com que as métricas das matrizes apareçam como valores ponderados aos restantes valores da matriz, assim cada valor da métrica relativa é dividida pelo somatório dos valores dessa métrica presentes na matriz."
      ],
      "metadata": {
        "id": "B36NBIsnCb2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def create_matrices(data_final, n = 12, fisrt_marker = 'volume', second_marker = 'vwap', third_marker = '', relative_fisrt_marker = True, relative_second_marker = False, relative_third_marker = False):\n",
        "    num_rows = data_final.shape[0]\n",
        "    num_matrices = num_rows // n  # Número de matrizes completas que podemos formar\n",
        "\n",
        "    matrices = []\n",
        "    matrices_y = []\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    for i in range(num_matrices):\n",
        "        start_idx = i * n\n",
        "        end_idx = start_idx + n\n",
        "        matrix = data_final.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        if relative_fisrt_marker:\n",
        "            # Calcular a soma da coluna de volume\n",
        "            matrix[fisrt_marker] = scaler.fit_transform(matrix[[fisrt_marker]]).flatten()\n",
        "\n",
        "            # Converter para valores numpy e adicionar à lista de matrizes\n",
        "            #matrices.append(matrix.values)\n",
        "\n",
        "        if relative_second_marker:\n",
        "            # Calcular a soma da coluna de volume\n",
        "            matrix[second_marker] = scaler.fit_transform(matrix[[second_marker]]).flatten()\n",
        "\n",
        "            # Converter para valores numpy e adicionar à lista de matrizes\n",
        "            #matrices.append(matrix.values)\n",
        "\n",
        "        if relative_third_marker:\n",
        "            # Calcular a soma da coluna de volume\n",
        "            matrix[third_marker] = scaler.fit_transform(matrix[[third_marker]]).flatten()\n",
        "\n",
        "            # Converter para valores numpy e adicionar à lista de matrizes\n",
        "        matrices.append(matrix.values)\n",
        "\n",
        "        # Criar labels\n",
        "        if i > 0:  # Ignorar a primeira matriz para alinhamento correto\n",
        "            first_value = matrices[i][0, 1]\n",
        "            last_value = matrices[i][n-1, 1]\n",
        "            delta = last_value - first_value\n",
        "            delta_bi = 1 if delta >= 0 else 0\n",
        "            matrices_y.append(delta_bi)\n",
        "\n",
        "    return matrices, matrices_y"
      ],
      "metadata": {
        "id": "8t2H56cVKuTT"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso se entenda que a ordem temporal, devido á elevada granuraridade dos dados de uma matriz, é irrelevante, esta função permite ordenar por ordem decrescente da métrica da segunda coluna os valores de uma matriz e assim por exemplo caso na segunda coluna se tenha o valor de volume estaria se a observar em cada matriz o preço por ordem crescente do volume. Esta função pode ser ativada ou desativada e pode ser interessante para uma elevada granuralidade em que as matrizes apenas representam 5 minutos e não se está a usar uma LSTM."
      ],
      "metadata": {
        "id": "PRnv-jgWIx1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_matrices(matrices):\n",
        "    # Ordenar as linhas de cada matriz por ordem decrescente da coluna Average (segunda coluna)\n",
        "    matrices_x = []\n",
        "    for matrix in matrices:\n",
        "        sorted_indices = np.argsort(matrix[:, 1])[::-1]\n",
        "        sorted_matrix = matrix[sorted_indices]\n",
        "        matrices_x.append(sorted_matrix)\n",
        "    return matrices_x"
      ],
      "metadata": {
        "id": "1hhFCRB2DCZK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funçaõ para passar as matrizes para tensores para poderem ser tartadas pelo TensorFlow"
      ],
      "metadata": {
        "id": "T81TdRROfxML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_creator(matrices_x, matrices_y):\n",
        "    # Converter listas para arrays numpy\n",
        "    matrices_x = np.array(matrices_x)\n",
        "    matrices_y = np.array(matrices_y)\n",
        "    return matrices_x, matrices_y"
      ],
      "metadata": {
        "id": "pRejyoiHDEhc"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função que integra todas as outras funções que falamos para obter o dataset e manipulá-lo conforme o utilizador do algoritmo deseja"
      ],
      "metadata": {
        "id": "dUWkAibmgI5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(company_data, n_rows, sort = True, fisrt_marker = 'volume', second_marker = 'vwap' ,\n",
        "                third_marker = '', relative_fisrt_marker = True, relative_second_marker = False,\n",
        "                relative_third_marker = False, decimals = ''):\n",
        "    matrices_x = []\n",
        "    matrices_y = []\n",
        "\n",
        "    matrices_x, matrices_y = create_matrices(company_data, n_rows, fisrt_marker, second_marker, third_marker, relative_fisrt_marker, relative_second_marker, relative_third_marker)\n",
        "\n",
        "    if sort:\n",
        "        matrices_x = sort_matrices(matrices_x)\n",
        "\n",
        "    # Remover a última matriz de matrices_x para alinhamento com matrices_y\n",
        "    if matrices_x:\n",
        "        matrices_x.pop()\n",
        "\n",
        "    matrices_x, matrices_y = tensor_creator(matrices_x, matrices_y)\n",
        "\n",
        "    if decimals != '':\n",
        "        matrices_x = np.round(matrices_x, decimals = decimals)\n",
        "    return matrices_x, matrices_y"
      ],
      "metadata": {
        "id": "IqP4yd62jdV4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função divide o dataset em treino, validação e teste. Esta divisão tem de ser feita de forma diferente caso se esteja a usar um LSTM ou não. Se for usado um LSTM para manter a ordem temporal dos dados o datset é dividido em fatias sequêncis primeiros 60% dos dados históricos para treino, proximos 20% para validação e ultimos 20% para teste. Se não for usado uma LSTM os dados históricos são formatados em matrizes, as matrizes são baralhadas e entregues baralhadas tempralmente ao modelo."
      ],
      "metadata": {
        "id": "mIYzhJoenbMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(matrices_x, matrices_y, LSTM = True):\n",
        "\n",
        "    if LSTM:\n",
        "        # Total de amostras\n",
        "        n_samples = matrices_x.shape[0]\n",
        "\n",
        "        # Índices para divisão\n",
        "        train_size = int(0.6 * n_samples)  # 60% para treino\n",
        "        val_size = int(0.2 * n_samples)    # 20% para validação\n",
        "        test_size = n_samples - train_size - val_size  # 20% para teste\n",
        "\n",
        "        # Dividir os dados sequencialmente\n",
        "        X_train = matrices_x[:train_size]\n",
        "        y_train = matrices_y[:train_size]\n",
        "\n",
        "        X_val = matrices_x[train_size:train_size + val_size]\n",
        "        y_val = matrices_y[train_size:train_size + val_size]\n",
        "\n",
        "        X_test = matrices_x[train_size + val_size:]\n",
        "        y_test = matrices_y[train_size + val_size:]\n",
        "    else:\n",
        "        # Dividir os dados em conjuntos de treinamento + validação e teste\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(matrices_x, matrices_y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Dividir o conjunto de treinamento + validação em treinamento e validação\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "id": "uAksoOB0q-_H"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função para criar o modelo de AI. Esta função permite escolher entre duas arquiteturas diferentes. É possível escolher entre usar uma LSTM, caso se queira dar prioridade á lógica temporal do mercado, ou usar uma arquitetura diferente caso não se queira dar essa relevância à ordem temporal.\n",
        "\n",
        "Pode ser interessante não usar uma LSTM. O padrão de procedimento na industria é usar um LSTM devido ao motivo lógico que esxite uma padrão temporal de acontecimentos obvios no mercado. No entanto, para explorar outras prespetivas de olhar para o mercado, desenvolves te várias ferramentas que em teoria permitiriam olhar para o mercado sem ser relevante a ordem temporal. A ideia é que pela teoria do mercado eficiente combiando com a tua ideia de um torque financeiro, que deve abrir um curto intervalo de tempo no mercado para ganhar lucro comprativamento com os outros players do mercado. Assim se aumentares por muito a granuralidade e assim estarás a fazer previsões para a matriz seguinte, ou seja, se as matrizes apenas representam 5 minutos no mercado, logo somando essa a ideia de uma granuralidade elevada com o facto de tornar os parametros na matriz relativos, e ordenar as linhas da matriz por ordem crecente dos valores e não tempora, todas estas ferramentas juntas em teoria permitiriam analisar o mercado sem ter em conta a ordem temporal.\n"
      ],
      "metadata": {
        "id": "YCc_bVslgY0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "\n",
        "# Função para construir a rede neural\n",
        "def build_model(input_shape, LSTM_boolean=True):\n",
        "    warnings.filterwarnings('ignore', category=UserWarning, module='keras')\n",
        "    model = Sequential()\n",
        "\n",
        "    if LSTM_boolean:\n",
        "        model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(64, return_sequences=False))\n",
        "        model.add(Dropout(0.2))\n",
        "    else:\n",
        "        model.add(Flatten(input_shape=input_shape))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compilando o modelo com uma taxa de aprendizado ajustada\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "VkdihWussdhK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função para treinar o modelo, usamos earlystopping para saber qunado para de forma mais correta as épocas. Esta função também grava o histórico de treinamento assim como também testa o modelo contra o dataset de teste e grava os resultados."
      ],
      "metadata": {
        "id": "QL1X35y0p7xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_training(model, X_train, X_val, X_test, y_train, y_val, y_test, epochs=5, early_stopping = True, call_back = 'val_accuracy', patience = 2, batch_size=32):\n",
        "\n",
        "    # Treinar o modelo\n",
        "    print(\"Treinando o modelo...\")\n",
        "\n",
        "    if early_stopping:\n",
        "        early_stopping = EarlyStopping(monitor=call_back, patience=patience, restore_best_weights=True)\n",
        "        history = model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "    else:\n",
        "        history = model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, validation_data=(X_val, y_val))\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Avaliar o modelo nos dados de teste\n",
        "    print(\"Avaliação no conjunto de teste:\")\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return model, history, test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "Fo-yl-ysvUC-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "função que permite dar plot dos resultados da historia de treinamento e dar plot aos resultado do modelo no dataset de teste."
      ],
      "metadata": {
        "id": "ZtI-19T3qYKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotar a precisão e a perda ao longo das épocas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def graphics_analysis(history):\n",
        "\n",
        "    # Precisão\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Perda\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "42-pWp7Gvz9F"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todas as variaveis que se podem perdsonalizar no algoritmo."
      ],
      "metadata": {
        "id": "6P7EmkqFqyvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As primeiras possibilidades de cada variável são aquelas as quais inicialmnete pensas-te que faziam mais sentido:\n",
        "\n",
        "* symbol = 'AAPL', (retira do ficheito execel, tens o código que extrai)\n",
        "* max_period = True, (podes ter erros por excesso de pedidos do API)\n",
        "* start_date = '', '2022-10-31', (1 ano funciona bem para test runs)\n",
        "* end_date = '', '2023-10-31', (1 ano funciona bem para test runs)\n",
        "* timeframe = '1Min', '5Min', (mais de 5Min parece perder sentido a teoria)\n",
        "* fisrt_marker = 'volume', 'close', 'high', 'low', 'trade_count', 'open', 'vwap', 'average_oc', 'average_hl', (a ideia inicial é ver o volume relativo, mas pode ser interessante estudar o trade_count)\n",
        "* second_marker = 'vwap', 'volume', 'close', 'high', 'low', 'trade_count', 'open', 'average_oc', 'average_hl', (a ideia inicial era ver a média entre o high e o low - 'average_hl', mas fica na duvida ver a média entre o open e o close - 'average_oc', pode também ser muito interessante investigar o vwap)\n",
        "* third_marker = '', 'volume', 'close', 'high', 'low', 'trade_count', 'open', 'vwap', 'average_oc', 'average_hl', (não pensas te em usar uma terceira coluna para criar as matrizes, mas se os resultados impiricos assim o provarem usa)\n",
        "* n_rows = 12, 10, 15, 30, 5, (pensa no timeframe quando fazes isso, o 12 pareceu ser o melhor em testes impiricos)\n",
        "* sort = True, (tens de pensar na sequencia temporal se faz sentido manter ou não)\n",
        "* relative_first_marker = True, (tens de pensar em que marker correspondente estás a usar, pensa se matemáticamente faz sentido ou não colocares o marcador com valores relativos para todo o tensor, faltam te dados impiricos, o volume pensas te inicialmnete deixar em impirico para o tensor para normalizares os tensores)\n",
        "* relative_second_marker = False, (tens de pensar em que marker correspondente estás a usar, pensa se matemáticamente faz sentido ou não colocares o marcador com valores relativos para todo o tensor, faltam te dados impiricos)\n",
        "* relative_third_marker = False, (tens de pensar em que marker correspondente estás a usar, pensa se matemáticamente faz sentido ou não colocares o marcador com valores relativos para todo o tensor, faltam te dados impiricos)\n",
        "* decimals = '', 3, 4 (tens de ver o que os dados impiricos te dizem se com arrendondamentos facilitas ou n a vida do AI)\n",
        "* LSTM_boolean = True, (em caso de falso não usas um LSTM e partes do precipio que a relação causal deixa de ser importante para a cosntrução do tensor, quando normalizas os dados a esperança seria de não teres de usar um LSTM)\n",
        "* epochs = 5, (este numero tende a ser irrelevante porque tens um checkpoint de callback referente ao melhor resultado de validation accuracy)\n",
        "* early_stopping = True, (podes ligar ou desligar de forma automática o early_stopping, ou seja, o call back aqui)\n",
        "* call_back = 'val_accuracy', 'val_loss', 'accuracy', 'loss' (escolhes o paramtero que monitora o call back)\n",
        "* patience = 2, 50, 100, 500, 1000 (escolhes a paciencia do call back)\n",
        "* batch_size = 64, 16, 32, 128, 256, 512 (varias com o tramanho do dataset, mas pensa que tens conseguido datasets bem grandes portanto usa os maiores valores)"
      ],
      "metadata": {
        "id": "d6KIcjmYWb5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbol = 'AAPL'\n",
        "max_period = False\n",
        "start_date = '2016-01-1'\n",
        "end_date = '2024-12-30'\n",
        "timeframe = '5Min'\n",
        "fisrt_marker = 'vwap'\n",
        "second_marker = 'trade_count'\n",
        "third_marker = ''\n",
        "n_rows = 36\n",
        "sort = False\n",
        "relative_first_marker = True\n",
        "relative_second_marker = False\n",
        "relative_third_marker = False\n",
        "decimals = ''\n",
        "LSTM_boolean = True\n",
        "epochs = 500\n",
        "early_stopping = True\n",
        "call_back = 'val_accuracy'\n",
        "patience = 100\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "CSPJtOTF8u58"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função que agrega todas as anteriores para preceber se com base nas variaveis introduzidas no algoritmo, o algoritmo já foi corrido alguma vez com estes settings ou não. Caso tenha sido ele vai buscar á google drive os dados guardados anteriormente com estes settings o que otimiza o tempo que demora a correr o algoritmo, pois caso contrário é necessário usar a API Alpaca para obter todos os dados o que pode demorar bastante tempo."
      ],
      "metadata": {
        "id": "XCsbuEmMs6LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "old_dataset, caminho_do_modelo = old_dataset_function(symbol, max_period, start_date, end_date,\n",
        "                                  timeframe, fisrt_marker, second_marker, third_marker,\n",
        "                                  n_rows, sort, relative_first_marker, relative_second_marker,\n",
        "                                  relative_third_marker, decimals, LSTM_boolean, epochs,\n",
        "                                  early_stopping, call_back, patience, batch_size, 0, 0, False)\n",
        "\n",
        "if old_dataset:\n",
        "    company_data = pd.read_csv(caminho_do_modelo, index_col=0)\n",
        "else:\n",
        "    company_data = get_all_data(symbol, max_period, start_date, end_date, timeframe)\n",
        "\n",
        "print(company_data)\n",
        "print(company_data.shape)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuIkfBkI-Uvw",
        "outputId": "8e66a040-1abc-41e9-ef7d-201e020b8493"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nome do arquivo mais próximo:\n",
            "s=AAPL+mp=False+sd=2016-01-1+ed=2024-12-30+tf=5Min.csv\n",
            "\n",
            "Caminho completo do modelo:\n",
            "/content/drive/MyDrive/AI Financial Analisys/Summer Project/Dataset/Raw Data/s=AAPL+mp=False+sd=2016-01-1+ed=2024-12-30+tf=5Min.csv\n",
            "\n",
            "Pontuação de similaridade:\n",
            "100\n",
            "                              close      high       low  trade_count  \\\n",
            "timestamp                                                              \n",
            "2016-01-01 00:00:00+00:00  105.4000  105.4300  105.4000            8   \n",
            "2016-01-01 00:10:00+00:00  105.4100  105.4100  105.4000            4   \n",
            "2016-01-01 00:15:00+00:00  105.4000  105.4100  105.4000            7   \n",
            "2016-01-01 00:35:00+00:00  105.4700  105.4700  105.4000           10   \n",
            "2016-01-01 00:45:00+00:00  105.5099  105.5100  105.4800           13   \n",
            "...                             ...       ...       ...          ...   \n",
            "2024-12-31 00:30:00+00:00  251.8689  251.8700  251.8300           33   \n",
            "2024-12-31 00:40:00+00:00  251.8682  251.8682  251.8682           18   \n",
            "2024-12-31 00:45:00+00:00  251.8500  251.8500  251.8500           10   \n",
            "2024-12-31 00:50:00+00:00  251.8400  251.8600  251.8300           44   \n",
            "2024-12-31 00:55:00+00:00  251.8700  251.8700  251.8000           60   \n",
            "\n",
            "                               open  volume        vwap  \n",
            "timestamp                                                \n",
            "2016-01-01 00:00:00+00:00  105.4300     865  105.407123  \n",
            "2016-01-01 00:10:00+00:00  105.4000     838  105.404289  \n",
            "2016-01-01 00:15:00+00:00  105.4100    1621  105.404203  \n",
            "2016-01-01 00:35:00+00:00  105.4100    1800  105.444645  \n",
            "2016-01-01 00:45:00+00:00  105.4800    1999  105.498727  \n",
            "...                             ...     ...         ...  \n",
            "2024-12-31 00:30:00+00:00  251.8300     965  251.863500  \n",
            "2024-12-31 00:40:00+00:00  251.8682     342  251.868200  \n",
            "2024-12-31 00:45:00+00:00  251.8500     398  251.850000  \n",
            "2024-12-31 00:50:00+00:00  251.8400    1456  251.845227  \n",
            "2024-12-31 00:55:00+00:00  251.8000    2538  251.858417  \n",
            "\n",
            "[462418 rows x 7 columns]\n",
            "(462418, 7)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função é chamada para fazer o tratamento dos dados brutos conforme as settings que o usuário decidiu antes dos dados serem alimentados ao modelo."
      ],
      "metadata": {
        "id": "SSWCEYoPtt_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_data_prepared = prepare_data(company_data, fisrt_marker, second_marker, third_marker)\n",
        "\n",
        "print(company_data_prepared)\n",
        "print(company_data_prepared.shape)\n",
        "print()"
      ],
      "metadata": {
        "id": "3EvayUzCP499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81005ee9-ca3f-49c6-87a6-fc93b39ec362"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 vwap  trade_count\n",
            "timestamp                                         \n",
            "2016-01-01 00:00:00+00:00  105.407123            8\n",
            "2016-01-01 00:10:00+00:00  105.404289            4\n",
            "2016-01-01 00:15:00+00:00  105.404203            7\n",
            "2016-01-01 00:35:00+00:00  105.444645           10\n",
            "2016-01-01 00:45:00+00:00  105.498727           13\n",
            "...                               ...          ...\n",
            "2024-12-31 00:30:00+00:00  251.863500           33\n",
            "2024-12-31 00:40:00+00:00  251.868200           18\n",
            "2024-12-31 00:45:00+00:00  251.850000           10\n",
            "2024-12-31 00:50:00+00:00  251.845227           44\n",
            "2024-12-31 00:55:00+00:00  251.858417           60\n",
            "\n",
            "[462418 rows x 2 columns]\n",
            "(462418, 2)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "esta função separa os dadaos em matrizes com as dimensões esplicitas nas settings do algoritmo."
      ],
      "metadata": {
        "id": "qwO0z4-RuBUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrices_x, matrices_y = get_dataset(company_data_prepared, n_rows, sort, fisrt_marker, second_marker , third_marker, relative_first_marker, relative_second_marker, relative_third_marker, decimals)\n",
        "\n",
        "print(matrices_x)\n",
        "print(matrices_x.shape)\n",
        "print(matrices_y)\n",
        "print(matrices_y.shape)\n",
        "print()"
      ],
      "metadata": {
        "id": "iejgU_5vP8KQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a302909b-7686-4f80-b1ef-93a4442aa4c8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 1.96366184e+00  8.00000000e+00]\n",
            "  [ 1.95989113e+00  4.00000000e+00]\n",
            "  [ 1.95977671e+00  7.00000000e+00]\n",
            "  ...\n",
            "  [-7.45908799e-01  8.00000000e+00]\n",
            "  [-8.99052097e-01  3.60000000e+01]\n",
            "  [-1.09359864e+00  3.20000000e+01]]\n",
            "\n",
            " [[ 7.52706781e-01  8.00000000e+00]\n",
            "  [ 4.35276939e-01  5.00000000e+00]\n",
            "  [-1.67857421e-01  2.20000000e+01]\n",
            "  ...\n",
            "  [-3.18415715e+00  1.64890000e+04]\n",
            "  [-4.17136931e-01  1.31950000e+04]\n",
            "  [ 5.71827795e-01  1.10080000e+04]]\n",
            "\n",
            " [[-1.28929401e+00  7.24500000e+03]\n",
            "  [-1.06559743e+00  6.47400000e+03]\n",
            "  [-1.42673499e+00  5.37600000e+03]\n",
            "  ...\n",
            "  [ 1.06004938e+00  2.95000000e+03]\n",
            "  [ 1.56403518e+00  4.00500000e+03]\n",
            "  [ 2.18549797e+00  3.90000000e+03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 6.08264161e-01  1.72000000e+02]\n",
            "  [ 5.09302396e-01  1.11000000e+02]\n",
            "  [ 4.48243405e-01  4.00000000e+01]\n",
            "  ...\n",
            "  [-2.54161775e+00  4.60000000e+02]\n",
            "  [-2.97111258e+00  2.20000000e+02]\n",
            "  [-3.07564050e+00  2.77000000e+02]]\n",
            "\n",
            " [[ 2.47397573e+00  2.30000000e+03]\n",
            "  [ 1.22059014e+00  2.43000000e+02]\n",
            "  [ 1.13934931e+00  2.46000000e+02]\n",
            "  ...\n",
            "  [-7.43581815e-01  6.49800000e+03]\n",
            "  [-1.09863175e+00  4.49000000e+03]\n",
            "  [-7.30615901e-01  4.13100000e+03]]\n",
            "\n",
            " [[-1.97129953e+00  6.29100000e+03]\n",
            "  [-2.20451176e+00  4.76400000e+03]\n",
            "  [-2.50259865e+00  5.60000000e+03]\n",
            "  ...\n",
            "  [ 9.69679705e-01  2.69100000e+03]\n",
            "  [ 8.50320355e-01  3.02300000e+03]\n",
            "  [ 9.86349486e-01  2.70200000e+03]]]\n",
            "(12843, 36, 2)\n",
            "[1 0 1 ... 1 0 0]\n",
            "(12843,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função divide o dataset em treino, teste e validação"
      ],
      "metadata": {
        "id": "tmPuLFl4uLKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(matrices_x, matrices_y, LSTM_boolean)\n",
        "\n",
        "# Exibir as formas dos arrays resultantes\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print()"
      ],
      "metadata": {
        "id": "edWtixMXP-QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13769951-b12d-4bff-d4d8-727c7b6424bf"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (7705, 36, 2)\n",
            "Shape of y_train: (7705,)\n",
            "Shape of X_val: (2568, 36, 2)\n",
            "Shape of y_val: (2568,)\n",
            "Shape of X_test: (2570, 36, 2)\n",
            "Shape of y_test: (2570,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este parte do código permite obter as características da arqitetura do AI que está a ser usada."
      ],
      "metadata": {
        "id": "0hDJwwbxuS9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if third_marker == '':\n",
        "  input_shape = (n_rows, 2)\n",
        "else:\n",
        "  input_shape = (n_rows, 3)\n",
        "model = build_model(input_shape, LSTM_boolean)"
      ],
      "metadata": {
        "id": "7k-Pt2rQQFiO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ca2659b5-8c28-4300-dec8-5d28f17de0b5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m17,152\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m33,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,241\u001b[0m (196.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,241</span> (196.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,241\u001b[0m (196.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,241</span> (196.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função treina o modelo nos dados de treino e validação. E testa o modelo contra os dados de teste."
      ],
      "metadata": {
        "id": "WB739fMbudj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, history, test_loss, test_accuracy = model_training(model, X_train, X_val, X_test, y_train, y_val, y_test, epochs, early_stopping, call_back, patience, batch_size)"
      ],
      "metadata": {
        "id": "yRLU8McBQKdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c967c0-9494-4b94-d7da-8bb02e762c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando o modelo...\n",
            "Epoch 1/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - accuracy: 0.7482 - loss: 0.5332 - val_accuracy: 0.8088 - val_loss: 0.4670\n",
            "Epoch 2/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.8158 - loss: 0.4383 - val_accuracy: 0.7858 - val_loss: 0.4665\n",
            "Epoch 3/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 144ms/step - accuracy: 0.8242 - loss: 0.4262 - val_accuracy: 0.8127 - val_loss: 0.4619\n",
            "Epoch 4/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.8260 - loss: 0.4260 - val_accuracy: 0.8150 - val_loss: 0.4453\n",
            "Epoch 5/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 144ms/step - accuracy: 0.8283 - loss: 0.4070 - val_accuracy: 0.7843 - val_loss: 0.4652\n",
            "Epoch 6/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 126ms/step - accuracy: 0.8185 - loss: 0.4210 - val_accuracy: 0.8224 - val_loss: 0.4405\n",
            "Epoch 7/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.8304 - loss: 0.4018 - val_accuracy: 0.8119 - val_loss: 0.4405\n",
            "Epoch 8/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 129ms/step - accuracy: 0.8356 - loss: 0.4062 - val_accuracy: 0.8201 - val_loss: 0.4406\n",
            "Epoch 9/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 142ms/step - accuracy: 0.8381 - loss: 0.4001 - val_accuracy: 0.7967 - val_loss: 0.4628\n",
            "Epoch 10/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 127ms/step - accuracy: 0.8249 - loss: 0.4095 - val_accuracy: 0.8220 - val_loss: 0.4400\n",
            "Epoch 11/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 133ms/step - accuracy: 0.8409 - loss: 0.3991 - val_accuracy: 0.8224 - val_loss: 0.4373\n",
            "Epoch 12/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 140ms/step - accuracy: 0.8369 - loss: 0.3891 - val_accuracy: 0.8205 - val_loss: 0.4315\n",
            "Epoch 13/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8371 - loss: 0.3972 - val_accuracy: 0.8174 - val_loss: 0.4447\n",
            "Epoch 14/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 142ms/step - accuracy: 0.8459 - loss: 0.3834 - val_accuracy: 0.8228 - val_loss: 0.4251\n",
            "Epoch 15/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 122ms/step - accuracy: 0.8426 - loss: 0.3849 - val_accuracy: 0.8228 - val_loss: 0.4231\n",
            "Epoch 16/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.8491 - loss: 0.3804 - val_accuracy: 0.8236 - val_loss: 0.4188\n",
            "Epoch 17/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.8483 - loss: 0.3755 - val_accuracy: 0.8244 - val_loss: 0.4313\n",
            "Epoch 18/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.8338 - loss: 0.4009 - val_accuracy: 0.8283 - val_loss: 0.4216\n",
            "Epoch 19/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 127ms/step - accuracy: 0.8379 - loss: 0.3892 - val_accuracy: 0.8259 - val_loss: 0.4237\n",
            "Epoch 20/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 118ms/step - accuracy: 0.8420 - loss: 0.3921 - val_accuracy: 0.8181 - val_loss: 0.4485\n",
            "Epoch 21/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.8405 - loss: 0.3845 - val_accuracy: 0.8228 - val_loss: 0.4316\n",
            "Epoch 22/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 122ms/step - accuracy: 0.8401 - loss: 0.3966 - val_accuracy: 0.8228 - val_loss: 0.4222\n",
            "Epoch 23/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.8407 - loss: 0.3865 - val_accuracy: 0.8174 - val_loss: 0.4478\n",
            "Epoch 24/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 164ms/step - accuracy: 0.8394 - loss: 0.3852 - val_accuracy: 0.7512 - val_loss: 0.5060\n",
            "Epoch 25/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 116ms/step - accuracy: 0.8320 - loss: 0.4049 - val_accuracy: 0.8209 - val_loss: 0.4383\n",
            "Epoch 26/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.8325 - loss: 0.3945 - val_accuracy: 0.8294 - val_loss: 0.4094\n",
            "Epoch 27/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8258 - loss: 0.4061 - val_accuracy: 0.8290 - val_loss: 0.4339\n",
            "Epoch 28/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.8404 - loss: 0.3881 - val_accuracy: 0.8127 - val_loss: 0.4514\n",
            "Epoch 29/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 140ms/step - accuracy: 0.8387 - loss: 0.3901 - val_accuracy: 0.8244 - val_loss: 0.4199\n",
            "Epoch 30/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8475 - loss: 0.3720 - val_accuracy: 0.8298 - val_loss: 0.3973\n",
            "Epoch 31/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 151ms/step - accuracy: 0.8375 - loss: 0.3972 - val_accuracy: 0.8232 - val_loss: 0.4167\n",
            "Epoch 32/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 140ms/step - accuracy: 0.8408 - loss: 0.3811 - val_accuracy: 0.8146 - val_loss: 0.4435\n",
            "Epoch 33/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8388 - loss: 0.3865 - val_accuracy: 0.8232 - val_loss: 0.4222\n",
            "Epoch 34/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.8445 - loss: 0.3707 - val_accuracy: 0.8228 - val_loss: 0.4273\n",
            "Epoch 35/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.8394 - loss: 0.3827 - val_accuracy: 0.8248 - val_loss: 0.4157\n",
            "Epoch 36/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 145ms/step - accuracy: 0.8388 - loss: 0.3697 - val_accuracy: 0.8290 - val_loss: 0.4131\n",
            "Epoch 37/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.8496 - loss: 0.3545 - val_accuracy: 0.8244 - val_loss: 0.4239\n",
            "Epoch 38/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 136ms/step - accuracy: 0.8403 - loss: 0.3672 - val_accuracy: 0.8181 - val_loss: 0.4267\n",
            "Epoch 39/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.8446 - loss: 0.3792 - val_accuracy: 0.8244 - val_loss: 0.4231\n",
            "Epoch 40/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8439 - loss: 0.3554 - val_accuracy: 0.8213 - val_loss: 0.4192\n",
            "Epoch 41/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.8454 - loss: 0.3643 - val_accuracy: 0.8255 - val_loss: 0.4118\n",
            "Epoch 42/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 144ms/step - accuracy: 0.8505 - loss: 0.3508 - val_accuracy: 0.8201 - val_loss: 0.4293\n",
            "Epoch 43/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8450 - loss: 0.3531 - val_accuracy: 0.8220 - val_loss: 0.4244\n",
            "Epoch 44/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.8412 - loss: 0.3616 - val_accuracy: 0.8248 - val_loss: 0.4213\n",
            "Epoch 45/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 106ms/step - accuracy: 0.8472 - loss: 0.3527 - val_accuracy: 0.8201 - val_loss: 0.4250\n",
            "Epoch 46/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.8474 - loss: 0.3555 - val_accuracy: 0.8213 - val_loss: 0.4235\n",
            "Epoch 47/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.8502 - loss: 0.3588 - val_accuracy: 0.8263 - val_loss: 0.4289\n",
            "Epoch 48/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.8579 - loss: 0.3392 - val_accuracy: 0.8213 - val_loss: 0.4245\n",
            "Epoch 49/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.8528 - loss: 0.3448 - val_accuracy: 0.8220 - val_loss: 0.4292\n",
            "Epoch 50/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 145ms/step - accuracy: 0.8467 - loss: 0.3533 - val_accuracy: 0.8213 - val_loss: 0.4253\n",
            "Epoch 51/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - accuracy: 0.8504 - loss: 0.3408 - val_accuracy: 0.8193 - val_loss: 0.4273\n",
            "Epoch 52/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 163ms/step - accuracy: 0.8439 - loss: 0.3470 - val_accuracy: 0.8220 - val_loss: 0.4247\n",
            "Epoch 53/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - accuracy: 0.8482 - loss: 0.3538 - val_accuracy: 0.8217 - val_loss: 0.4236\n",
            "Epoch 54/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 127ms/step - accuracy: 0.8450 - loss: 0.3457 - val_accuracy: 0.8158 - val_loss: 0.4492\n",
            "Epoch 55/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 142ms/step - accuracy: 0.8440 - loss: 0.3609 - val_accuracy: 0.8232 - val_loss: 0.4200\n",
            "Epoch 56/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 117ms/step - accuracy: 0.8445 - loss: 0.3583 - val_accuracy: 0.8267 - val_loss: 0.4259\n",
            "Epoch 57/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 129ms/step - accuracy: 0.8480 - loss: 0.3562 - val_accuracy: 0.8248 - val_loss: 0.4230\n",
            "Epoch 58/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 115ms/step - accuracy: 0.8539 - loss: 0.3435 - val_accuracy: 0.8271 - val_loss: 0.4173\n",
            "Epoch 59/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.8579 - loss: 0.3332 - val_accuracy: 0.8181 - val_loss: 0.4407\n",
            "Epoch 60/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 124ms/step - accuracy: 0.8469 - loss: 0.3459 - val_accuracy: 0.8224 - val_loss: 0.4275\n",
            "Epoch 61/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - accuracy: 0.8436 - loss: 0.3591 - val_accuracy: 0.8244 - val_loss: 0.4297\n",
            "Epoch 62/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 106ms/step - accuracy: 0.8506 - loss: 0.3389 - val_accuracy: 0.8298 - val_loss: 0.4283\n",
            "Epoch 63/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 142ms/step - accuracy: 0.8545 - loss: 0.3325 - val_accuracy: 0.8252 - val_loss: 0.4298\n",
            "Epoch 64/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8497 - loss: 0.3425 - val_accuracy: 0.8143 - val_loss: 0.4413\n",
            "Epoch 65/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.8455 - loss: 0.3669 - val_accuracy: 0.8267 - val_loss: 0.4238\n",
            "Epoch 66/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 132ms/step - accuracy: 0.8529 - loss: 0.3269 - val_accuracy: 0.8255 - val_loss: 0.4197\n",
            "Epoch 67/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.8500 - loss: 0.3503 - val_accuracy: 0.8181 - val_loss: 0.4409\n",
            "Epoch 68/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 105ms/step - accuracy: 0.8548 - loss: 0.3363 - val_accuracy: 0.8252 - val_loss: 0.4274\n",
            "Epoch 69/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.8534 - loss: 0.3322 - val_accuracy: 0.8294 - val_loss: 0.4213\n",
            "Epoch 70/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 147ms/step - accuracy: 0.8458 - loss: 0.3416 - val_accuracy: 0.8224 - val_loss: 0.4175\n",
            "Epoch 71/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 115ms/step - accuracy: 0.8519 - loss: 0.3290 - val_accuracy: 0.8232 - val_loss: 0.4245\n",
            "Epoch 72/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 184ms/step - accuracy: 0.8531 - loss: 0.3198 - val_accuracy: 0.8298 - val_loss: 0.4141\n",
            "Epoch 73/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 155ms/step - accuracy: 0.8543 - loss: 0.3277 - val_accuracy: 0.8248 - val_loss: 0.4103\n",
            "Epoch 74/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 104ms/step - accuracy: 0.8585 - loss: 0.3318 - val_accuracy: 0.8228 - val_loss: 0.4029\n",
            "Epoch 75/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.8453 - loss: 0.3589 - val_accuracy: 0.8224 - val_loss: 0.4261\n",
            "Epoch 76/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - accuracy: 0.8505 - loss: 0.3407 - val_accuracy: 0.8201 - val_loss: 0.4351\n",
            "Epoch 77/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.8511 - loss: 0.3421 - val_accuracy: 0.8220 - val_loss: 0.4284\n",
            "Epoch 78/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 121ms/step - accuracy: 0.8515 - loss: 0.3317 - val_accuracy: 0.8259 - val_loss: 0.4230\n",
            "Epoch 79/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 139ms/step - accuracy: 0.8595 - loss: 0.3211 - val_accuracy: 0.8220 - val_loss: 0.4296\n",
            "Epoch 80/500\n",
            "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.8530 - loss: 0.3358 - val_accuracy: 0.8224 - val_loss: 0.4352\n",
            "Epoch 81/500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plota os resultados durante a fase de treinamento e durante a fase de teste."
      ],
      "metadata": {
        "id": "1thPzlJJvDta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graphics_analysis(history)"
      ],
      "metadata": {
        "id": "Dl7PDLgBtW66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso o modelo seja corrido em settings originais ou caso o modelo apresente resultados inovadores o modelo será gravado usando esta função e os settings que influenciram o modelo e os resultados obtidos pelo modelo são encriptados no nome do ficheiro no qual o modelo fica guardado."
      ],
      "metadata": {
        "id": "45o5Ea1axTw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Defina o caminho do diretório no Google Drive\n",
        "caminho_para_salvar_modelo = '/content/drive/MyDrive/AI Financial Analisys/Summer Project/Neural Network/Models'\n",
        "\n",
        "# Verifique se o diretório existe, caso contrário, crie-o\n",
        "if not os.path.exists(caminho_para_salvar_modelo):\n",
        "    print(f\"O dirétório {caminho_para_salvar_modelo} não existe.\")\n",
        "\n",
        "file_name = create_file_name(symbol, max_period, start_date, end_date,\n",
        "                     timeframe, fisrt_marker, second_marker, third_marker,\n",
        "                     n_rows, sort, relative_first_marker, relative_second_marker,\n",
        "                     relative_third_marker, decimals, LSTM_boolean, epochs,\n",
        "                     early_stopping, call_back, patience, batch_size,\n",
        "                     test_loss, test_accuracy, True)\n",
        "\n",
        "caminho_completo_modelo = os.path.join(caminho_para_salvar_modelo, file_name)\n",
        "\n",
        "# Salva o modelo em formato\n",
        "model.save(caminho_completo_modelo)\n",
        "\n",
        "print(\"Ficheiro do Modelo:\")\n",
        "print(file_name)\n",
        "print()\n",
        "print(\"Modelo salvo em:\")\n",
        "print(caminho_para_salvar_modelo)"
      ],
      "metadata": {
        "id": "ukvgCcVq2KhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso os dados sejam coletados por uma versão do algoritmo que correu em settings originais os dados seram gravados usando esta função e os settings que influenciram o levaram a obeter os respetivos dados/dataset modelo são encriptados no nome do ficheiro CSV no qual os dados ficam guardados."
      ],
      "metadata": {
        "id": "NOFMb-rxx8HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Defina o caminho do diretório no Google Drive\n",
        "caminho_para_salvar_dados = '/content/drive/MyDrive/AI Financial Analisys/Summer Project/Dataset/Raw Data'\n",
        "\n",
        "# Verifique se o diretório existe, caso contrário, crie-o\n",
        "if not os.path.exists(caminho_para_salvar_dados):\n",
        "    os.makedirs(caminho_para_salvar_dados)\n",
        "    print(f\"Diretório {caminho_para_salvar_dados} criado com sucesso.\")\n",
        "else:\n",
        "    print(f\"O diretório {caminho_para_salvar_dados} já existe.\")\n",
        "\n",
        "print()\n",
        "\n",
        "file_name = create_file_name(symbol, max_period, start_date, end_date,\n",
        "                     timeframe, fisrt_marker, second_marker, third_marker,\n",
        "                     n_rows, sort, relative_first_marker, relative_second_marker,\n",
        "                     relative_third_marker, decimals, LSTM_boolean, epochs,\n",
        "                     early_stopping, call_back, patience, batch_size,\n",
        "                     test_loss, test_accuracy, False)\n",
        "\n",
        "# Defina o caminho completo para salvar o arquivo\n",
        "caminho_completo_dados = os.path.join(caminho_para_salvar_dados, file_name)\n",
        "\n",
        "# Salve o dataframe no caminho especificado\n",
        "company_data.to_csv(caminho_completo_dados)\n",
        "\n",
        "print(\"Ficheiro dos Dados:\")\n",
        "print(file_name)\n",
        "print()\n",
        "print(\"Dados salvos em:\")\n",
        "print(caminho_para_salvar_dados)"
      ],
      "metadata": {
        "id": "dDtkMLFeDQRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}